<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Building a RAG Pipeline</title>
    <link rel="stylesheet" href="../css/styles.css">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/themes/prism-okaidia.min.css" rel="stylesheet" />
    <link rel="stylesheet" href="../css/blog-styles.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/plugins/autoloader/prism-autoloader.min.js"></script>
</head>
<body class="blog-page">
    <nav>
        <a href="../index.html">home</a>
        <a href="../blog.html">blog</a>
        <a href="../projects.html">work</a>
    </nav>
    <main class="blog-content">
        <h1>Building a RAG Pipeline using gemma-2B</h1>
        <p class="date">Sept 10, 2024</p>

        <h2 id="introduction">Introduction</h2>
        <p>Recently, I got my hands dirty building a RAG Pipeline from scratch, and I have to say, it's an amazing way to handle large text documents. I worked with an open-source Human Nutrition textbook and transformed it into a system where I can query the document, retrieve relevant information, and generate intelligent answers using an LLM (Large Language Model).</p>
        <p>Here's a detailed breakdown of the entire process, and some real outputs!</p>

        <h3 id="preparing-and-reading-the-pdf">Preparing and Reading the PDF</h3>
        <p>The first step was extracting text from the PDF. I used PyMuPDF for reading the PDF because it's fast and handles large documents like a champ. The document used was the Human Nutrition 2020 Edition.</p>
        <p>Here's the code to get the text from each page and store it in a structured way:</p>
        <pre><code class="language-python">def open_pdf(pdf_path: str):
    doc = fitz.open(pdf_path)
    pages_and_texts = []

    for page_number, page in enumerate(doc):
        text = page.get_text()
        pages_and_texts.append({"page_number": page_number, "text": text.strip()})

    return pages_and_texts

pages_and_texts = open_pdf("human-nutrition-text.pdf")
        </code></pre>
        <p>Here's a sample output from one of the extracted pages:</p>
        <pre><code class="language-output">Page 84:

"The cardiovascular system is one of the eleven organ systems of the human body. Its main function is to transport nutrients to cells and wastes from cells. This system consists of the heart, blood, and blood vessels. The heart pumps the blood, and the blood is the transportation fluid."
        </code></pre>

        <h3 id="chunking-the-text-into-pieces">Chunking the Text into Pieces</h3>
        <p>Since embedding models like Sentence-BERT have a limit on how much text they can handle (usually around 384 tokens), Thus splitting the PDF text into smaller, manageable chunks.</p>
        <p>For chunking, I split the text into groups of sentences using spaCy.
        This makes sure we don't overload the model with too many tokens at once.</p>
        <pre><code class="language-python">import spacy
nlp = spacy.load("en_core_web_sm")

def chunk_text(text, chunk_size=10):
    doc = nlp(text)
    sentences = [sent.text for sent in doc.sents]
    return [sentences[i:i + chunk_size] for i in range(0, len(sentences), chunk_size)]
        </code></pre>

        <h3 id="embedding-the-chunks">Embedding the Chunks</h3>
        <p>Each chunk of text is then converted into numerical representations (embeddings) using Sentence-BERT. These embeddings allow for efficient similarity searches based on the meaning of the text.</p>
        <pre><code class="language-python">from sentence_transformers import SentenceTransformer

# Load pre-trained Sentence-BERT model
model = SentenceTransformer('paraphrase-MiniLM-L6-v2')

# Embedding the text chunks
embeddings = [model.encode(chunk) for page in pages_and_texts for chunk in page['chunks']]
        </code></pre>
        <p>This step produces vector representations of the chunks, which can be used to find relevant information during retrieval. A sample of an embedding: This was only a portion of the output, lol.</p>
        <pre><code class="language-output">Sample Embedding Vector (truncated):
[0.123, -0.245, 0.003, 0.157, ..., 0.056]
        </code></pre>

        <h3 id="retrieving-relevant-chunks">Retrieving Relevant Chunks</h3>
        <p>To retrieve relevant chunks, cosine similarity is used to compare the query's embedding with the document embeddings. The most similar chunks (based on vector closeness) are returned. </p>
        <pre><code class="language-python">from sklearn.metrics.pairwise import cosine_similarity

def search(query, embeddings):
    query_embedding = model.encode(query)
    similarities = cosine_similarity([query_embedding], embeddings)[0]
    top_n_indices = similarities.argsort()[-5:][::-1] 
    return top_n_indices
        </code></pre>
        <p>For example, when querying "What is the cardiovascular system?":</p>
        <pre><code class="language-python">query = "What is the cardiovascular system?"
results = search(query, embeddings)

# Retrieve the most relevant chunks
for idx in results:
    print(pages_and_texts[idx]['chunks'])
        </code></pre>

        <pre><code class="language-output">Retrieved Chunk 1:
"The cardiovascular system is one of the eleven organ systems of the human body. Its main function is to transport nutrients to cells and wastes from cells."

Retrieved Chunk 2:
"This system consists of the heart, blood, and blood vessels. The heart pumps the blood, and the blood is the transportation fluid."
        </code></pre>
 
        <h3 id="setting-up-the-generation-with-a-llm">Setting Up the Generation with a LLM</h3>
        <p>Now that the retrieval pipeline is ready, the next step is to add generation functionality using a Large Language Model (LLM). The goal here is to generate context-aware answers based on the information retrieved from the document.</p>
        <p>To do this, we'll use the Gemma model, which can run on a local GPU. The model will take a query and the relevant context as input, generating a response that answers the question while considering the extracted information.</p>
        <pre><code class="language-python">import torch
device = "cuda" if torch.cuda.is_available() else "cpu"
gpu_memory_bytes = torch.cuda.get_device_properties(0).total_memory
gpu_memory_gb = round(gpu_memory_bytes / (2**30))

if gpu_memory_gb < 5.1:
    model_id = "google/gemma-2b"
    use_quantization_config = True
elif gpu_memory_gb < 8.1:
    model_id = "google/gemma-2b-it"
    use_quantization_config = True
elif gpu_memory_gb < 19.0:
    model_id = "google/gemma-2b-it"
    use_quantization_config = False
else:
    model_id = "google/gemma-7b-it"
    use_quantization_config = False

print(f"Model selected: {model_id}")
        </code></pre>
        <pre><code class="language-output">Model selected: google/gemma-2b-it
        </code></pre>
        <p>Loading the Model and Tokenizer</p>
        <p>Once the right model is selected, it can be loaded with or without quantization, depending on the available GPU memory. </p>
        <pre><code class="language-python">from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16) if use_quantization_config else None
tokenizer = AutoTokenizer.from_pretrained(model_id)

llm_model = AutoModelForCausalLM.from_pretrained(model_id, 
                                                 torch_dtype=torch.float16, 
                                                 quantization_config=quantization_config)
        </code></pre>

        <h3 id="generating-responses-based-on-query">Generating Responses Based on Query</h3>
        <p>After retrieving relevant text from the document, the next step is to generate responses. The Gemma model, fine-tuned on instruction-based data, requires that the input prompt be formatted correctly, often in a conversational template.
        Formatting the Prompt with Context</p>
        <p>The prompt includes both the user's query and relevant context retrieved from the text. Here's how to format it: </p>
        <pre><code class="language-python">def prompt_formatter(query: str, context_items: list[dict]) -> str:
    # Combine context items into a single paragraph
    context = "- " + "\n- ".join([item["sentence_chunk"] for item in context_items])

    # Base prompt with examples
    base_prompt = f"""Based on the following context items, please answer the query.
Relevant passages: &lt;extract relevant passages from the context here&gt;
User query: {query}
Answer:"""

    # Format dialogue template for instruction-tuned model
    dialogue_template = [{"role": "user", "content": base_prompt}]

    # Apply chat template for the model
    return tokenizer.apply_chat_template(conversation=dialogue_template, tokenize=False, add_generation_prompt=True)
        </code></pre>
        <p><strong>Example: Augmenting Query with Context</strong></p>
        <p>Here's an example query, formatted with context: </p>
        <pre><code class="language-python">query = "What role does fiber play in digestion?"
context_items = [  # Retrieved relevant chunks
    {"sentence_chunk": "Fiber helps digestion by regulating bowel movements and maintaining gut health."},
    {"sentence_chunk": "It adds bulk to stool and prevents constipation."}
]

# Format the prompt
formatted_prompt = prompt_formatter(query, context_items)
print(formatted_prompt)
        </code></pre>
        <pre><code class="language-output">Based on the following context items, please answer the query.
Relevant passages: 
- Fiber helps digestion by regulating bowel movements and maintaining gut health.
- It adds bulk to stool and prevents constipation.
User query: What role does fiber play in digestion?
Answer:
        </code></pre>
        <p><strong>Generating the Response</strong></p>
        <p>With the formatted prompt ready, the Gemma model generates an answer using the relevant context: </p>
        <pre><code class="language-python">
input_ids = tokenizer(formatted_prompt, return_tensors="pt").to("cuda")
outputs = llm_model.generate(input_ids, max_new_tokens=256, temperature=0.7)

# Decode the output to readable text
response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(response)
        </code></pre>
        <p>Sample output for the query "What role does fiber play in digestion?" might look like:</p>
        <pre><code class="language-output">"Fiber plays a crucial role in digestion by promoting regular bowel movements, preventing constipation, and maintaining gut health. It adds bulk to stool and facilitates smoother passage through the digestive tract."
        </code></pre>

        <h3 id="putting-it-all-together">Putting it All Together</h3>
        <p>At this point, the RAG pipeline is complete. The system can:</p>
        <ol>
        <li>Retrieve relevant information from a large text document (like a      textbook).</li>
        <li>Generate human-like answers to queries by incorporating context from the retrieved information. </li>
        </ol>
        <pre><code class="language-python">def ask(query: str, max_new_tokens=512):
    # Retrieve relevant chunks
    scores, indices = retrieve_relevant_resources(query=query, embeddings=embeddings)
    context_items = [pages_and_chunks[i] for i in indices]

    # Format the prompt
    prompt = prompt_formatter(query=query, context_items=context_items)

    # Generate an answer
    input_ids = tokenizer(prompt, return_tensors="pt").to("cuda")
    outputs = llm_model.generate(input_ids, max_new_tokens=max_new_tokens)

    # Return the answer
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# Test the complete pipeline
query = "What are the benefits of vitamin C?"
print(ask(query))
        </code></pre>
        <p>Sample generated answer for the query "What are the benefits of vitamin C?": </p>
        <pre><code class="language-output">"Vitamin C is essential for the growth and repair of tissues in the body. It plays a role in collagen production, wound healing, and maintaining healthy skin and blood vessels. Vitamin C is also a powerful antioxidant, helping to protect cells from oxidative stress and boosting the immune system."
        </code></pre>

        <h2 id="conclusion">Conclusion</h2>
        <p>Woah! The complete RAG pipelineâ€”retrieval, augmentation, and generation all happening on your own GPU! </p>
        <p>This setup opens the door to a variety of use cases, such as:</p>
        <ol>
        <li>Customer support: Answering questions based on a knowledge base.</li>
        <li>Research assistance: Extracting and generating summaries from scientific papers.</li>
        <li>Interactive textbooks: Turning large textbooks into a Q&A system. </li>
        </ol>

        <h2 id="references-guide">References &amp; Guide</h2>
        <p>Daniel Brouke - <a href="https://www.youtube.com/watch?v=qN_2fnOPY-M">Youtube Link</a><br>Langchain documentation - <a href="https://github.com/langchain-ai/rag-from-scratch">Github repo</a> </p>
    </main>

    <footer>
        <hr>
        <section class="footer-links">
            <h3>find me elsewhere online:</h3>
            <ul class="links">
                <li><a href="mailto:sreedeepek.95@gmail.com">email<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M7 7h10v10"></path><path d="M7 17 17 7"></path></svg></a></li>
                <li><a href="https://x.com/sreedeepEK">x.com<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M7 7h10v10"></path><path d="M7 17 17 7"></path></svg></a></li>
                <li><a href="https://github.com/sreedeepEK">github<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M7 7h10v10"></path><path d="M7 17 17 7"></path></svg></a></li>
                <li><a href="https://www.linkedin.com/in/sreedeepek/">linkedin<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M7 7h10v10"></path><path d="M7 17 17 7"></path></svg></a></li>
            </ul>
        </section>
    </footer>

    <script src="js/script.js"></script>
</body>
</html>